## 算法说明

### 预处理

BoW（Bag of Words）词袋：由特征值构成的向量和标签的组合。

先从现有的训练数据的文本中只提取出名词，忽略名词在文本中的顺序，把它们作为集合，作为列名

每一条训练数据都可以看做一行数据（向量），对应了每个列名出现的次数。

假设我们有两段短文本：

1. “我喜欢机器学习”
2. “机器学习很有趣”

首先，我们对这两段文本进行分词：

- 文本 1 分词结果：`["我", "喜欢", "机器", "学习"]`
- 文本 2 分词结果：`["机器", "学习", "很", "有趣"]`

创建词汇表：

接着，我们把所有的词汇都列出来，形成一个词汇表：

```
["我", "喜欢", "机器", "学习", "很", "有趣"]
```

生成词频向量：

然后，我们可以用这个词汇表来表示每一段文本，通过记录词汇表中每个词在该文本中出现的次数（即词频）来生成向量表示。

- 文本 1 的向量：`[1, 1, 1, 1, 0, 0]`
- 文本 2 的向量：`[0, 0, 1, 1, 1, 1]`

### 概率的计算

在分类时求出每个标签对应的概率，将概率最高的标签作为分类结果。朴素贝叶斯在训练时计算以下两种概率。

1. 每个标签（类型）出现的概率。
2. 在各标签（类型）下，每个单词出现的条件概率。

平滑：是一种平衡的方式，将没有出现的单词概率0改为小的概率值如：0.01

朴素贝叶斯认为所有词的出现都是独立事件：

因此分别为每个标签值计算每个标签条件概率的乘积
