# 朴素贝叶斯

## 概述

朴素贝叶斯（Naive Bayes）是常用于自然语言分类问题的算法。其计算数据为某个标签的概率，并将其分类为概率值最大的标签。

根据词频进行分类，也就是自然语言中，根据文本中内容不同类型词频，来判断这个文本属于哪个类别。

在预测数据类别时，朴素贝叶斯会计算特定单词在各个类别中的条件概率。

例如，对于验证数据中的“感动”一词，虽然在不同类别（如“电影”和“宇宙”）都会出现，但在每个类别中出现的概率不同：感动在电影的三条数据中出现两次，在宇宙的三条数据中只出现一次。

这表明“感动”一词在“电影”类别中更常见。

并且，还记录了条件概率：在出现某个标签的情况下，为某个类型的概率

例如：在出现了“感动”一词的情况下，是电影的概率是66%

朴素贝叶斯综合利用文本中每个单词的条件概率，假设单词的出现是相互独立的，即一个单词的出现不会影响其他单词的出现概率。

最终，通过单词的条件概率累积，求出文本在不同类别下的概率值，将文本归到概率最高的类别。

## 算法说明

### 预处理

BoW（Bag of Words）词袋：由特征值构成的向量和标签的组合。

先从现有的训练数据的文本中只提取出名词，忽略名词在文本中的顺序，把它们作为集合，作为列名

每一条训练数据都可以看做一行数据（向量），对应了每个列名出现的次数。

假设我们有两段短文本：

1. “我喜欢机器学习”
2. “机器学习很有趣”

首先，我们对这两段文本进行分词：

- 文本 1 分词结果：`["我", "喜欢", "机器", "学习"]`
- 文本 2 分词结果：`["机器", "学习", "很", "有趣"]`

创建词汇表：

接着，我们把所有的词汇都列出来，形成一个词汇表：

```
["我", "喜欢", "机器", "学习", "很", "有趣"]
```

生成词频向量：

然后，我们可以用这个词汇表来表示每一段文本，通过记录词汇表中每个词在该文本中出现的次数（即词频）来生成向量表示。

- 文本 1 的向量：`[1, 1, 1, 1, 0, 0]`
- 文本 2 的向量：`[0, 0, 1, 1, 1, 1]`

### 概率的计算

在分类时求出每个标签对应的概率，将概率最高的标签作为分类结果。朴素贝叶斯在训练时计算以下两种概率。

1. 每个标签（类型）出现的概率。
2. 在各标签（类型）下，每个单词出现的条件概率。

平滑：是一种平衡的方式，将没有出现的单词概率0改为小的概率值如：0.01

朴素贝叶斯认为所有词的出现都是独立事件：

因此分别为每个标签值计算每个标签条件概率的乘积

## 示例代码

```python
from sklearn.naive_bayes import MultinomialNB
# 生成数据
X_train = [[1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0],
           [0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
           [1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0],
           [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0],
           [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0],
           [0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1]]
y_train = [1, 1, 1, 0, 0, 0]
model = MultinomialNB()
# 训练
model.fit(X_train, y_train)
# 预测新数据
y_pred = model.predict(X_train)
y_pred_prob = model.predict_proba(X_train)
model.predict([[1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0]])
```

## 详细说明

朴素贝叶斯是自然语言分类的基础算法，但是不适合预测天气预报中的降水概率那种预测值是概率本身的情况。

朴素贝叶斯所有词的出现都是独立事件，忽略了单词之间的关联性，若单词关联性强，应当考虑其他模型