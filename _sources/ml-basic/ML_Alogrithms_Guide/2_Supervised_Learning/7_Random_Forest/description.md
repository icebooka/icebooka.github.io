## 算法说明

### 决策树

决策树是将训练数据进行划分的方法。

决策树的输入通常是 特征矩阵 与 标签向量，相当于数据内容和分类结果。

训练数据的杂乱程度称为不纯度。利用不纯度来比较分割方法的优劣。

表示不纯度的具体指标有很多，如基尼系数。

基尼系数 = $1-\sum_{i=1}^c p_i^2$（c是标签数，pi是某标签的数量在总数的占比）

所示为使用几种不同的分割方法计算得出的 加权平均基尼系数（区域内的基尼系数乘以区域在总数占比）

左侧是分割前的状态。右侧是使基尼系数的平均值最小的分割方法的例子。

![image.png](images/2.png)

决策树便是通过反复分割来进行的。

步骤：

- 计算某个区域的所有特征值和候选分割的不纯度.
- 以分割时不纯度减小最多的分割方式分割区域。
- 对于分割后的区域，重复步骤1和步骤2。
    
![image.png](images/3.png)

### 随机森林

随机森林如何使用多个决策树来提高正确率：

- 假设每个决策树的正确率都是60%
- 对于训练后决策树，每次的输入都会给定一个答案，n个决策树都给出正确率为60%的答案
- 服从多数正确，最后的正确率会大于60%

决策树如何在相同数据下独立：

- Bootstrap方法：每个决策树获得的样本集是 n 次的 有放回的随机抽取 集合（也就是单个数据可能重复）。
- 随机选取特征值：对于每个节点的分裂，不使用全部特征，而是随机选择一个特征子集来决定分裂。
  
  ![image.png](images/4.png)

随机森林利用这种方式创建多棵数据集、训练多棵决策树、对预测结果进行多数表决，返回最终的分类结果。
