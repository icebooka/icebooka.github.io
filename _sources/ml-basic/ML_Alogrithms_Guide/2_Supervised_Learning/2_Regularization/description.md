## 算法说明

岭回归（Ridge Regression）是一种线性回归的改进方法，常用于解决多重共线性问题（即解释变量之间高度相关）和防止模型过拟合。

之所以复杂模型会出现过拟合：学习参数值太大或太小。

随着学习此处的增加，学习参数的绝对值会变大，但使用了正则化则会减少这种情况。

### 岭回归的误差函数

考虑对二次线性回归应用正则化的情况：

$R(w) = \sum_{i=1}^{m} \left[ y_i - (w_0 + w_1 x_i + w_2 x_i^2) \right]^2 + \alpha (w_1^2 + w_2^2)$

第1项 $\sum_{i=1}^{m} \left[ y_i - (w_0 + w_1 x_i + w_2 x_i^2) \right]^2$ 是线性回归的损失函数。

第2项 $\alpha (w_1^2 + w_2^2)$ 被称为惩罚项（或者正则化项），是学习参数的平方和的形式。

一般来说，惩罚项中不包含截距。

α控制了正则化强度，α越大，对学习参数的抑制就越强。

### 损失函数最小化

岭回归的误差函数就是在后面加上了惩罚项，距我们之前所说，造成过拟合的原因是w值的绝对值过大，因此如果w值的绝对值过大，就增加惩罚项，从而避免过拟合。

用于抑制学习参数。
