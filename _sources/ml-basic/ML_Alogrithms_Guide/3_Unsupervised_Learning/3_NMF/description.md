## 算法说明

### 实例

假设我们有以下三篇文档：

- 文档1：谈论人工智能和机器学习。
- 文档2：讨论深度学习和神经网络。
- 文档3：介绍数据分析和统计学。

通过 NMF，可以将文档表示为某些“主题”的线性组合。例如：

- 主题A：与“人工智能、机器学习”相关。
- 主题B：与“统计学、数据分析”相关。
- 主题C：与“深度学习、神经网络”相关。

**NMF 的结果**可能如下：

- 文档1 ≈ 0.6 × 主题A + 0.3 × 主题C
- 文档2 ≈ 0.8 × 主题C + 0.1 × 主题A
- 文档3 ≈ 0.7 × 主题B + 0.2 × 主题A

这个表示让我们可以用**主题分布**来描述每篇文档，比如：

- 文档1的“主题A贡献”为0.6，“主题C贡献”为0.3。

### 抽象化

设原始数据为$n$行$d$列的矩阵$V$。将其表示为两个矩阵$W$和$H$的乘积。

- $W$是$n$行$r$列的矩阵，是一个 文档 - 主题 矩阵每一行表示一个文档在不同主题上的权重。
- $H$是$r$行$d$列的矩阵，每一列表示一个主题在各个词汇上的权重。
- $WH$是原始矩阵$V$的近似，选择比$d$小的$r$就可以进行降维。

![2.png](images/2.png)

在求$W$和$H$的过程中，NMF在$W ≥ 0、H ≥ 0$的条件下，使$WH$接近$V$。

NMF采取“将$H$视为常数，更新$W$”“ 将$W$视为常数，更新$H$”的方式交替更新$W$和$H$.

以下对NMF过程进行可视化。

灰色的点为原始矩阵$V$，绿色的点为近似矩阵$WH$。随着计算的进行，我们可以看到近似矩阵越来越接近原始矩阵。此外，红线和蓝线是潜在空间的轴，所有近似矩阵的图形都能在潜在空间（二维空间）的轴上表示出来

- 将$W$和$H$初始化为正值。

- 将$H$视为常数，更新$W$。

- 将$W$视为常数，更新$H$。

- 当$W$和$H$收敛时，停止计算。

  ![3.png](images/3.png)
